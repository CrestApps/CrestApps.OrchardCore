---
sidebar_label: 2.0.0 Release Notes
sidebar_position: 1
title: "Version 2.0.0 Release Notes"
description: Release notes for CrestApps.OrchardCore 2.0.0 — new modules, breaking changes, and migration guide.
---

# Version 2.0.0 Release Notes

**Package version**: `2.0.0-preview-0001`

This is a major release that introduces new modules, an orchestrator-based architecture, Omnichannel Communications, expanded MCP protocol support, and document handling capabilities.

---

## New Modules

### AI Chat Interactions

- **Feature ID**: `CrestApps.OrchardCore.AI.Chat.Interactions`
- Provides ad-hoc chat sessions with configurable parameters — users can adjust model settings, attach documents, upload images, and generate charts without requiring a predefined AI profile.

### AI Chat Copilot Integration

- **Feature ID**: `CrestApps.OrchardCore.AI.Chat.Copilot`
- Integrates with the GitHub Copilot SDK to provide a Copilot-based orchestrator for AI completions.

### AI Documents

A suite of modules for document upload, text extraction, and embedding during chat sessions:

| Module | Feature ID |
|--------|------------|
| AI Documents (Core) | `CrestApps.OrchardCore.AI.Documents` |
| PDF Support | `CrestApps.OrchardCore.AI.Documents.Pdf` |
| OpenXml Support (Word, Excel, PowerPoint) | `CrestApps.OrchardCore.AI.Documents.OpenXml` |
| Azure AI Search | `CrestApps.OrchardCore.AI.Documents.AzureAI` |
| Elasticsearch | `CrestApps.OrchardCore.AI.Documents.Elasticsearch` |

### AI Data Sources

Modules for RAG (Retrieval-Augmented Generation) and knowledge base indexing:

| Module | Feature ID |
|--------|------------|
| AI Data Sources (Core) | `CrestApps.OrchardCore.AI.DataSources` |
| Azure AI Search | `CrestApps.OrchardCore.AI.DataSources.AzureAI` |
| Elasticsearch | `CrestApps.OrchardCore.AI.DataSources.Elasticsearch` |

### MCP Resource Adapters

| Module | Feature ID |
|--------|------------|
| FTP Resources | `CrestApps.OrchardCore.AI.Mcp.Resources.Ftp` |
| SFTP Resources | `CrestApps.OrchardCore.AI.Mcp.Resources.Sftp` |

### Omnichannel Communications

A new suite of modules for multi-channel communication:

| Module | Feature ID | Description |
|--------|------------|-------------|
| Omnichannel (Core) | `CrestApps.OrchardCore.Omnichannel` | Core omnichannel services |
| SMS | `CrestApps.OrchardCore.Omnichannel.Sms` | AI-driven SMS automation with Twilio |
| Event Grid | `CrestApps.OrchardCore.Omnichannel.EventGrid` | Azure Event Grid webhook integration |
| Management | `CrestApps.OrchardCore.Omnichannel.Managements` | Contact and conversation management |

### Recipes Module

- **Feature ID**: `CrestApps.OrchardCore.Recipes`
- Provides recipe steps for configuring CrestApps modules via recipes.

---

## New Features in Existing Modules

### AI Services (`CrestApps.OrchardCore.AI`)

- **Orchestrator Architecture** — The new `IOrchestrator` interface replaces the previous prompt routing system. The orchestrator manages planning, tool scoping, and iterative agent execution loops.
- **AI Tool Registration** — New fluent API for registering AI tools with `.AddAITool<T>()`, supporting categories, purposes, and selectable/system tool modes.
- **AI Profile Types** — Added `Utility` and `TemplatePrompt` profile types in addition to `Chat`.
- **AI Deployments** — New feature for managing AI model deployments.
- **AI Connection Management** — UI for managing provider connections from the admin dashboard.
- **AI Chat WebAPI** — RESTful API endpoints for interacting with AI chat.
- **Workflow Integration** — AI Completion tasks for Orchard Core Workflows.

### MCP (`CrestApps.OrchardCore.AI.Mcp`)

- **MCP Server** — Expose your Orchard Core site as an MCP server endpoint, allowing external AI agents to discover and use your tools, prompts, and resources.
- **MCP Prompts and Resources** — Prompts and resources can be added and managed via the admin UI.
- **Templated Resources** — Support for dynamic MCP resources defined with URI templates.
- **Stdio Transport** — Connect to local MCP servers (e.g., Docker containers) via Standard Input/Output.
- **Template URI Whitespace Handling** — Resource URI templates and incoming URIs are now trimmed of leading/trailing whitespace before matching, preventing mismatches caused by accidental spaces in URI definitions.
- **File Resource Directory Rejection** — The file resource handler now returns a descriptive error when the resolved path is a directory instead of a file, rather than attempting to read directory content.

### AI Agent (`CrestApps.OrchardCore.AI.Agent`)

- Expanded toolset with 30+ built-in tools covering content management, tenant management, feature toggles, workflow automation, and communication tasks.

---

## Improvements

### Unified Citation & Reference System

The citation and reference system has been completely reworked so that **every AI provider** (Azure OpenAI, OpenAI, Ollama, Azure AI Inference) now returns the same citation references. Previously, citations only worked with Azure OpenAI's native data-sources feature (`GetMessageContext()`); since we now inject context ourselves via preemptive RAG and tool-based search, that approach no longer applied.

**What changed:**

- **`[doc:N]` citation markers** are now produced consistently by both Data Source and Document preemptive RAG handlers, as well as by the `DataSourceSearchTool` and `SearchDocumentsTool` AI tools.
- **`referenceType`** is stored in the knowledge base index so the system knows whether a reference is a Content item, an uploaded Document, or a custom data source type.
- **`AICompletionReference`** now includes `ReferenceId` and `ReferenceType` properties, enabling downstream consumers (hubs, UI) to generate appropriate links.
- **`IAIReferenceLinkResolver`** — a new keyed-service interface for resolving reference links by type. Register custom resolvers with `services.AddKeyedScoped<IAIReferenceLinkResolver, MyResolver>("MyType")` to generate links for custom reference types.
- **`CompositeAIReferenceLinkResolver`** dispatches to the correct keyed resolver based on `referenceType`. When no resolver is registered, the reference is shown without a link.
- **`CitationReferenceCollector`** collects references from all sources (preemptive RAG context, tool-invoked searches) and resolves links in a single pass.
- **Content item link resolution** — `DefaultAILinkGenerator` is registered as a keyed `IAIReferenceLinkResolver` for the `"Content"` reference type. Content item references automatically receive links generated via OrchardCore's `LinkGenerator` with the standard `OrchardCore.Contents` route. Document references (uploaded files) are shown by filename without a link.
- **`DocumentChunkSearchResult`** now includes `DocumentKey` and `FileName` properties for uploaded document citation tracking.
- **Azure OpenAI**: Removed the deprecated `GetMessageContext()` / `Citations` extraction logic and the `IAILinkGenerator` dependency from `AzureOpenAICompletionClient`. References are now handled uniformly via the orchestration pipeline.
- **`AIInvocationScope` / `AIInvocationContext`** — new `AsyncLocal<T>`-based ambient context that replaces `HttpContext.Items` for all per-invocation AI data. This ensures full isolation between concurrent SignalR hub calls on the same WebSocket connection, preventing reference leaks, stale data source IDs, and other cross-invocation contamination issues. See the [AI Tools documentation](../ai/ai-tools.md#invocation-context-aiinvocationscope) for details.
- **Shared reference counter** — `AIInvocationContext.NextReferenceIndex()` provides a monotonically increasing, thread-safe counter used by all preemptive RAG handlers and search tools, ensuring `[doc:N]` indices never collide even when data source and document references are produced in the same request.
- **Incremental citation delivery** — Citation references are now collected and sent to the client progressively during streaming. Preemptive RAG references (from data sources and documents) are resolved before the streaming loop starts, so the first chunk already includes them. Tool-invoked references are merged incrementally during streaming as tools execute. This ensures the JavaScript client can render `[doc:N]` superscripts in real-time.
- **`ChatSession` moved to `Items` dictionary** — `AIInvocationContext.ChatSession` property has been removed. The chat session is now stored in `AIInvocationContext.Items["AIChatSession"]` instead. Tools that need the chat session should read from `Items` (e.g., `invocationContext.Items.TryGetValue("AIChatSession", out var session)`).
- **IsInScope evaluation moved to orchestrator** — The `IsInScope` constraint is no longer evaluated by individual preemptive RAG handlers (`DataSourcePreemptiveRagHandler`, `DocumentPreemptiveRagHandler`). Instead, the `PreemptiveRagOrchestrationHandler` evaluates it after all handlers have run. When no references are produced across all sources and `IsInScope` is enabled, a scoping directive is injected. When tools are available, the directive encourages the model to try tool-based search before concluding no answer exists, allowing `search_data_source` and `search_documents` to discover relevant content that the initial preemptive search missed.
- **Tool-search instructions when preemptive RAG is disabled** — When preemptive RAG is disabled but data sources or documents are attached, the orchestrator now injects system-message instructions guiding the model to call search tools (`search_data_source`, `search_documents`) to retrieve internal knowledge. When `IsInScope` is enabled, the model is forced to use only tool-retrieved content and must refuse to answer from general knowledge. When `IsInScope` is disabled, the model is instructed to try the search tools first and may supplement with general knowledge only if no relevant results are found.
- **RAG text normalization** — Content and titles are now normalized before chunking and embedding using `RagTextNormalizer`. HTML tags, Markdown formatting, escaped HTML entities, and extraneous whitespace are stripped to produce clean plain text. This improves embedding quality, reduces token usage when injecting context into prompts, and prevents raw HTML from leaking into reference titles and chat UI. Normalization uses [`Microsoft.Extensions.DataIngestion.Markdig`](https://www.nuget.org/packages/Microsoft.Extensions.DataIngestion.Markdig) for structured Markdown-to-plain-text conversion, combined with HTML tag stripping and entity decoding. Titles in citation references are also normalized at creation time as a defense-in-depth measure for existing indexed data.
- **Token-aware chunking** — The custom character-based text chunking has been replaced with `DocumentTokenChunker` from [`Microsoft.Extensions.DataIngestion`](https://www.nuget.org/packages/Microsoft.Extensions.DataIngestion). This uses actual LLM tokenizers (GPT-4o `o200k_base`) to split content at token boundaries with configurable overlap, producing chunks that align better with embedding model token limits.
- **`IngestionDocumentReader`-based document parsing** — The custom `IDocumentTextExtractor` interface has been replaced with [`Microsoft.Extensions.DataIngestion.IngestionDocumentReader`](https://learn.microsoft.com/dotnet/api/microsoft.extensions.dataingestion.ingestiondocumentreader), the standard abstraction from `Microsoft.Extensions.DataIngestion`. Each document module now provides an `IngestionDocumentReader` implementation registered as a keyed singleton by file extension. The built-in `MarkdownReader` from `Microsoft.Extensions.DataIngestion.Markdig` is used for Markdown files. Custom PDF and OpenXml readers extend `IngestionDocumentReader` following the same patterns used in Microsoft's AI templates. Use `services.AddIngestionDocumentReader<T>(extensions)` to register custom readers — this replaces the previous `AddDocumentTextExtractor<T>()` method.
- **Inline citation markers** — The system prompt now instructs the AI model to include `[doc:N]` reference markers inline in its response text, immediately after the relevant statement. This enables users to see which statements are sourced from which references.
- **Context-gated system tools** — The `search_data_sources` and document processing tools (`search_documents`, `list_documents`, `read_document`, `read_tabular_data`) are now conditionally included in the tool registry based on context availability. `search_data_sources` is only available when a data source is attached to the AI profile or chat interaction. Document processing tools are only available when documents are attached to the session. This prevents the AI model from seeing tools it cannot use, eliminating hallucinated tool calls and reducing token overhead. The `SystemToolRegistryProvider` checks `AICompletionContext.DataSourceId` for data sources and `AICompletionContextKeys.HasDocuments` in `AdditionalProperties` for documents.
- **Data source deletion cleanup** — When a data source is deleted, all associated document chunks are now properly removed from the master knowledge base index via `IDataSourceVectorSearchService.DeleteByDataSourceIdAsync`. Elasticsearch uses native `DeleteByQuery`; Azure AI Search uses filter-based pagination with batch deletion. The cleanup runs as a background job after the HTTP response completes to avoid blocking the admin UI. Previously, `DeleteDataSourceDocumentsAsync` resolved the document index manager but never invoked any deletion.
- **Chat session document cleanup** — When an AI chat session is deleted (single or bulk), all uploaded session documents are now cleaned up: `AIDocument` records are deleted from the document store, and their vector index chunks are removed via a deferred task from all AI document index profiles. Previously, session deletion only removed the session record, leaving orphaned documents and index entries.
- **Chat interaction document cleanup** — When a chat interaction is deleted, the `AIDocument` records associated with it are now deleted from the document store in addition to the vector index chunk cleanup that was already in place. Previously, only the index chunks were removed, leaving the document store records orphaned.
- **Strengthened tool-search instructions** — When preemptive RAG is disabled and data sources or documents are attached, the system prompt now uses mandatory language (`MUST call... BEFORE generating any response`) to ensure the AI model calls search tools before answering. Previously, advisory language was used which some models ignored. When `IsInScope` is off, the model is instructed to search first and may fall back to general knowledge only if no results are found.
- **Standardized prompt instruction format** — All AI system prompts now use a consistent `[Section Header]` + numbered rules format across the codebase. This includes `[Rules]` for utility prompts (chart generation, data extraction, search query extraction, post-session analysis, tabular processing), `[Output Format]` for expected output examples, and the existing `[Scope Constraint]`, `[Knowledge Source Instructions]`, and `[Response Guidelines]` for RAG-related prompts.
- **Sequential reference display indices** — The JavaScript clients (`ai-chat.js`, `chat-interaction.js`) now remap cited reference indices to a sequential 1-based sequence when rendering. If the model cites `[doc:2]` and `[doc:5]` but not `[doc:1]`, the user sees superscripts **1** and **2** (not 2 and 5), with the reference list numbered accordingly. This prevents confusing gaps in visible numbering. The remapping uses a two-phase placeholder approach to avoid collisions during index substitution.

**Breaking:** If you relied on `chunk.AdditionalProperties["ContentItemIds"]` or `chunk.AdditionalProperties["References"]` being set on streaming chunks by the Azure OpenAI provider, these are no longer set on individual chunks. References are now collected progressively during streaming from the orchestration context and tool execution context.

**Breaking:** If you wrote custom AI tools that read `AIToolExecutionContext` from `HttpContext.Items[nameof(AIToolExecutionContext)]`, update them to use `AIInvocationScope.Current?.ToolExecutionContext` instead. Similarly, if you read `HttpContext.Items["DataSourceId"]` or `HttpContext.Items["ToolSearchReferences"]`, these are now on `AIInvocationScope.Current.DataSourceId` and `AIInvocationScope.Current.ToolReferences` respectively.

**Breaking:** If you accessed `AIInvocationContext.ChatSession`, use `AIInvocationScope.Current?.Items["AIChatSession"]` instead.

**Breaking:** The `IDocumentTextExtractor` interface has been removed. If you implemented a custom document text extractor, migrate it to an `IngestionDocumentReader` subclass and register it with `services.AddIngestionDocumentReader<T>(extensions)` instead of `AddDocumentTextExtractor<T>(extensions)`. The reader's `ReadAsync` method returns an `IngestionDocument` instead of a raw string — the document processing service extracts text from the `IngestionDocument` automatically.

**Note:** Data sources and documents indexed before this release may contain raw HTML or Markdown in their content and titles. To benefit from normalization, re-index your data sources after upgrading.

---

### Performance: Reduced String Allocations with ZString

- **Adopted [ZString](https://github.com/Cysharp/ZString)** — Replaced `System.Text.StringBuilder` with ZString's `Utf16ValueStringBuilder` across AI system message generation, RAG context building, tool summaries, streaming response accumulation, CSV export, and batch processing. ZString uses `ArrayPool<char>` pooled buffers instead of allocating new internal arrays, significantly reducing GC pressure in hot paths.
- **Key areas converted**: `DefaultMcpMetadataPromptGenerator`, `DefaultOrchestrator`, `CopilotOrchestrator`, `DocumentOrchestrationHandler`, `DocumentPreemptiveRagHandler`, `DataSourcePreemptiveRagOrchestrationHandler`, `SearchDocumentsTool`, `DataSourceSearchTool`, `TabularBatchProcessor`, `AIChatHub`, `ChatInteractionHub`, `ChatAnalyticsController`, `ApiAICompletionEndpoint`, `GenerateImageTool`, and others.
- **Benchmark project added** — A new BenchmarkDotNet project at `tests/CrestApps.OrchardCore.Benchmarks` measures the allocation difference across five representative scenarios (system message generation, RAG context, streaming, CSV export, tool summaries). Run with `dotnet run -c Release --project tests/CrestApps.OrchardCore.Benchmarks`.

**Note:** Code that passes `StringBuilder` parameters to `DefaultMcpMetadataPromptGenerator.AppendParameterSummary` must update to `ref Utf16ValueStringBuilder`. The `Utf16ValueStringBuilder` is a disposable struct — always use `using var sb = ZString.CreateStringBuilder();` to return pooled buffers.

---

### Performance: AI Chat Session Prompt Storage Separation

- **Prompts moved to a dedicated document store** — `AIChatSessionPrompt` objects are now stored as separate YesSql documents instead of being embedded in the `AIChatSession` document. This dramatically reduces the data loaded when listing sessions (admin pages, widgets, API), since only session metadata (title, sessionId, profileId, status) is fetched without loading potentially large prompt histories.
- **Index-only session listings** — `DefaultAIChatSessionManager.PageAsync` now uses `QueryIndex<AIChatSessionIndex>` instead of `Query<AIChatSession, AIChatSessionIndex>`, returning lightweight `AIChatSessionEntry` DTOs. This is especially impactful for the admin widget which runs on every admin page request.
- **New `IAIChatSessionPromptStore`** — A new store interface (`IAIChatSessionPromptStore`) provides `GetPromptsAsync`, `DeleteAllPromptsAsync`, and `CountAsync` operations for session prompts.
- **Automatic data migration** — Existing sessions are automatically migrated: prompts are extracted from legacy `AIChatSession` documents into separate `AIChatSessionPrompt` documents via a batched deferred task (50 sessions per batch). No manual intervention is required.
- **`ChatMessageCompletedContext.Prompts`** — The handler context now includes a `Prompts` property with the loaded prompts, so `IAIChatSessionHandler` implementations can access prompts without an additional store query.

**Breaking:** `AIChatSession.Prompts` property has been removed. Code that previously accessed `session.Prompts` must now use `IAIChatSessionPromptStore.GetPromptsAsync(sessionId)` to load prompts.

**Breaking:** `AIChatSessionPrompt` now extends `CatalogItem`. The `Id` property has been replaced by `ItemId` (inherited from `CatalogItem`), and a `SessionId` property has been added to associate prompts with their session.

**Breaking:** `AIChatSessionResult.Sessions` now returns `IEnumerable<AIChatSessionEntry>` instead of `IEnumerable<AIChatSession>`. `AIChatSessionEntry` is a lightweight DTO containing only `SessionId`, `ProfileId`, `Title`, `UserId`, `ClientId`, `Status`, `CreatedUtc`, and `LastActivityUtc`.

**Breaking:** `PostSessionProcessingService.ProcessAsync` and `DataExtractionService.ProcessAsync` now require an `IReadOnlyList<AIChatSessionPrompt>` parameter. Callers must load prompts from the store and pass them explicitly.

**Breaking:** `AIChatSessionEventService.RecordSessionEndedAsync` now takes an `int promptCount` parameter instead of reading from `session.Prompts.Count`.

---

## Breaking Changes

### Changed: Navigation Paths

Orchard Core v3 removed the **Configuration** tab. Update any documentation or code that references:

- `Configuration → Features` → Use **Tools → Features**
- `Configuration → Settings` → Use **Settings** directly

### Changed: Package Version Scheme

Package versions now use the `2.0.0-preview-XXXX` scheme instead of `2.0.0-beta-XXXX`.

---

## Migration Guide from v1.x

### Step 1: Update Package References

Update all CrestApps package references to `2.0.0-preview-0001` or later:

```xml
<PackageReference Include="CrestApps.OrchardCore.Cms.Core.Targets" Version="2.0.0-preview-0001" />
```

### Step 2: Remove Prompt Routing Code

If you used `AddPromptProcessingIntent`, `IPromptIntentDetector`, or `IPromptProcessingStrategy`, remove these references. The orchestrator now handles all request processing automatically.

### Step 3: Update Tool Registrations

If you registered custom AI tools, update to the new fluent API:

```csharp
// Old (v1.x)
// services.AddSingleton<AIFunction, MyTool>();

// New (v2.0)
services.AddAITool<MyTool>(MyTool.TheName)
    .WithTitle("My Tool")
    .WithDescription("Description for the orchestrator")
    .Selectable();
```

### Step 4: Enable New Features

New modules are not enabled by default. Enable them via **Tools → Features** in the admin dashboard as needed.
